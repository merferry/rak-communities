\subsection{Experimental Setup}
\label{sec:setup}

We employ a server equipped with two Intel Xeon Gold 6226R processors, each featuring $16$ cores running at a clock speed of $2.90$ GHz. Every core is equipped with a $1$ MB L1 cache, a $16$ MB L2 cache, and shares a $22$ MB L3 cache. The system has $376$ GB of memory and runs on CentOS Stream 8. We use GCC 8.5 and OpenMP 4.5. Table \ref{tab:dataset} displays the graphs utilized in our experiments, all sourced from the SuiteSparse Matrix Collection \cite{suite19}.

\input{src/tab-dataset}
\input{src/fig-rak-compare}
\input{src/fig-louvainrak-compare}




\subsection{Comparing Performance of GVE-LPA}

We now compare the performance of GVE-LPA with igraph LPA \cite{csardi2006igraph}, FLPA \cite{traag2023large}, and NetworKit LPA \cite{staudt2016networkit}. igraph LPA and FLPA are both sequential implementations, while NetworKit LPA is parallel. For FLPA, we checkout the suitable branch with modified \texttt{igraph\_community\_label\_propagation()} function, update the label propagation example in C to load the input graph from a file and measure runtime of \texttt{igraph\_community\_label\_propagation} \texttt{()} invoked with \texttt{IGRAPH\_LPA\_FAST} variant, using \texttt{gettimeofday()}. For igraph LPA, we follow a similar process as FLPA, but checkout code from the \texttt{master} branch of igraph. For NetworKit, we employ a Python script to invoke \texttt{PLP} (Parallel Label Propagation), and measure the total time reported with \texttt{getTiming()}. For each graph, we measure the NetworKit LPA runtime five times for averaging. Due to time constraints and long running times of igraph LPA, we ran it only once per graph. Additionally, we record the modularity of communities obtained from each implementation.

Figure \ref{fig:rak-compare--runtime} shows the runtimes of FLPA, igraph LPA, NetworKit LPA, and GVE-LPA for each graph in the dataset, while Figure \ref{fig:rak-compare--speedup} shows the speedup of GVE-LPA with respect to each aforementioned implementation. igraph LPA fails to run on protein k-mer graphs, i.e., \textit{kmer\_A2a} and \textit{kmer\_V1r}, and hence its results on these graphs are not shown. GVE-LPA exhibits an average speedup of $139\times$, $97000\times$, and $40\times$ over FLPA, igraph LPA, and NetworKit LPA respectively. While FLPA and igraph LPA are sequential implementations, GVE-LPA, running on $64$ threads, is more than $64\times$ faster than these implementations. On the \textit{sk-2005} graph, GVE-LPA identifies communities in $2.7$ seconds, achieving a processing rate of $1.4$ billion edges/s. Figure \ref{fig:rak-compare--modularity} shows the modularity of communities obtained by each implementation. On average, GVE-LPA obtains $6.6\%$ / $0.2\%$ higher modularity than FLPA (especially on road networks) and igraph LPA respectively, and $4.1\%$ lower modularity than NetworKit LPA (especially on protein k-mer graphs\ignore{, we plan to address this issue in the future}).

\input{src/fig-rak-hardness}
\input{src/fig-rak-ss}

Next, we proceed to compare the performance of GVE-LPA with GVE-Louvain (our parallel implementation of Louvain algorithm) in Figure \ref{fig:louvainrak-compare}. Figures \ref{fig:louvainrak-compare--runtime}, \ref{fig:louvainrak-compare--speedup}, and \ref{fig:louvainrak-compare--modularity} present the runtimes, speedup (of GVE-LPA with respect to GVE-Louvain), and modularity of GVE-Louvain and GVE-LPA for each graph in the dataset. GVE-LPA offers a mean speedup of $5.4\times$ compared to GVE-Louvain, particularly on social networks, road networks, and protein k-mer graphs, while achieving on average $10.9\%$ lower modularity, especially on social networks and protein k-mer graphs.

In Figure \ref{fig:rak-hardness}, we assess the runtime per edge of GVE-LPA. Results indicate that GVE-LPA exhibits a higher $\text{runtime}/|E|$ factor on graphs with a low average degree (road networks and protein k-mer graphs) and on graphs with a weakly connected clusters (\textit{com-LiveJournal} and \textit{com-Orkut} graphs).




\subsection{Strong Scaling of GVE-LPA}

Finally, we assess the strong scaling performance of GVE-LPA. To this end, we vary the number of threads from $1$ to $64$ in multiples of $2$ for each input graph, and measure the time required for GVE-LPA to identify communities (averaged over five runs). Figure \ref{fig:rak-ss} shows the average strong-scaling of of GVE-LPA, in comparison with GVE-Louvain. With 32 threads, GVE-LPA achieves a speedup of $13.5\times$ compared to single-threaded execution, indicating a performance increase of $1.7\times$ for each doubling of threads. At 64 threads, GVE-LPA is moderately impacted by NUMA effects, and offers a speedup of $18.1\times$. This contrasts with GVE-Louvain, which appears to be significantly affected by NUMA effects.
